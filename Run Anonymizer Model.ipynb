{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrate the privacy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from main import init_seed # used by dataloaders\n",
    "from time import sleep\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('resnet.pkl', 'rb') as f:\n",
    "    resnet = pickle.load(f).cuda()\n",
    "with open('unet.pkl', 'rb') as f:\n",
    "    unet = pickle.load(f).cuda()\n",
    "    \n",
    "with open('test_action.pkl', 'rb') as f:\n",
    "    test_action = pickle.load(f)\n",
    "# with open('test_privacy.pkl', 'rb') as f:\n",
    "#     test_privacy = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/.conda/envs/skele/lib/python3.9/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 300, 25, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = next(iter(test_action))\n",
    "d[0] = d[0].to('cuda')\n",
    "d[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ = resnet.forward(d[0])\n",
    "unet_ = resnet.forward(d[0])\n",
    "# res_,unet_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 300, 25, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_action#, res_, unet_, d,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to convert their data to the way used by RF and SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_gen.rotation import *\n",
    "import data_gen.rotation\n",
    "from data_gen.preprocess import pre_normalization\n",
    "from data_gen.ntu_gendata import read_xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = read_xyz('/mnt/c/Users/Carrt/LocalCode/Skeleton-anonymization/data/raw_data/ntu60/S001C001P001R001A001.skeleton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2181153, 0.1725972, 3.785547)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[0][0][0][0], r[1][0][0][0], r[2][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 150)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def anonymizer_to_sgd(t, max_frames=300):\n",
    "    # (300, 150)\n",
    "    # [x:0,y:1,z:2][300][joints][actors]\n",
    "    xyz,frames,joints,actors = t.shape\n",
    "    # transpose to make loop simple\n",
    "    # [frame][actors][joints][xyz]\n",
    "    t = t.transpose(1,3,2,0)\n",
    "    # make empty array\n",
    "    frames = []\n",
    "    \n",
    "    joints_per_frame = xyz*joints*actors\n",
    "    \n",
    "    # crazy loop\n",
    "    for frame in t:\n",
    "        f = []\n",
    "        for actor in frame:\n",
    "            for joint in actor:\n",
    "                for xyz in joint:\n",
    "                    f.append(xyz)\n",
    "        \n",
    "        # Pad 0's to 150 joints (2 actors)\n",
    "        if len(f) < joints_per_frame:\n",
    "            f = np.pad(f, (0, joints_per_frame-len(f)), 'constant')\n",
    "            \n",
    "        frames.append(f)\n",
    "        \n",
    "    # to numpy array\n",
    "    X = np.array(frames, dtype=np.float32)\n",
    "    \n",
    "    if X.shape[0] < max_frames:\n",
    "        X = np.pad(X, ((0, max_frames-X.shape[0]), (0, 0)), 'constant')\n",
    "        \n",
    "    return X\n",
    "\n",
    "X = anonymizer_to_sgd(r)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X, r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the validation data and anonymize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/ntu120/xview/val_data_joint.npy', 'rb') as f:\n",
    "    X = np.load(f)\n",
    "with open('data/ntu120/xview/val_label.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)\n",
    "# with open('data/ntu120/xview/train_data_joint.npy', 'rb') as f:\n",
    "#     X2 = np.load(f)\n",
    "# with open('data/ntu120/xview/train_label.pkl', 'rb') as f:\n",
    "#     y2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict to hold all the data\n",
    "X_unet = {}\n",
    "X_resnet = {}\n",
    "\n",
    "def run_split(X, y):\n",
    "    # Convert X to torch tensor\n",
    "    X = torch.tensor(X)\n",
    "    \n",
    "    # Split data into batches\n",
    "    batch_size = 512\n",
    "    X = torch.split(X, batch_size, dim=0)\n",
    " \n",
    "    i = 0\n",
    "    with torch.no_grad():  # Use torch.no_grad() to reduce memory usage\n",
    "        for batch in tqdm(X):\n",
    "            # Move batch to cuda\n",
    "            batch_ = batch.cuda()\n",
    "    \n",
    "            # Get the anonymized data\n",
    "            unet_res = unet.forward(batch_)#.cpu().numpy()\n",
    "            resnet_res = resnet.forward(batch_)#.cpu().numpy()\n",
    "    \n",
    "            # Loop through actors in batch\n",
    "            for x in range(len(batch_)):\n",
    "                # Get the actor\n",
    "                actor = y[0][i][8:12]\n",
    "                i += 1\n",
    "\n",
    "                # If actor not in dict, add it\n",
    "                if actor not in X_unet:\n",
    "                    X_unet[actor] = []\n",
    "                    X_resnet[actor] = []\n",
    "                    \n",
    "                # Attempted fix to running out of CUDA memory\n",
    "           \n",
    "                # Apply unet and resnet privacy models\n",
    "                X_unet[actor].append(unet_res[x].cpu().numpy())\n",
    "                X_resnet[actor].append(resnet_res[x].cpu().numpy())\n",
    "      \n",
    "                # If actor not in dict, add it\n",
    "                # if actor not in X_unet:\n",
    "                #     X_unet[actor] = np.zeros((10000,3,300,25,2))\n",
    "                #     X_resnet[actor] = np.zeros((10000,3,300,25,2))\n",
    "           \n",
    "                # # Apply unet and resnet privacy models\n",
    "                # ind = np.where(X_unet[actor][:,0,0,0,0] == 0)[0][0]\n",
    "                # X_unet[actor][ind] = unet_res[x]\n",
    "                # X_resnet[actor][ind] = resnet_res[x]\n",
    "              \n",
    "            \n",
    "            # Clear GPU memory and CPU memory after each batch\n",
    "            del batch, batch_\n",
    "            del unet_res, resnet_res\n",
    "            torch.cuda.empty_cache()\n",
    "        del X\n",
    "    \n",
    "    # Trim zeros from each actor\n",
    "#     for actor in X_unet:\n",
    "#         ind = np.where(X_unet[actor][:,0,0,0,0] == 0)[0][0]\n",
    "#         X_unet[actor] = X_unet[actor][:ind]\n",
    "#         X_resnet[actor] = X_resnet[actor][:ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 75/75 [17:47<00:00, 14.24s/it]\n"
     ]
    }
   ],
   "source": [
    "run_split(X, y)\n",
    "del X, y\n",
    "# run_split(X2, y2)\n",
    "# del X2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to pickle\n",
    "with open('X_unet.pkl', 'wb') as f:\n",
    "    pickle.dump(X_unet, f)\n",
    "with open('X_resnet.pkl', 'wb') as f:\n",
    "    pickle.dump(X_resnet, f)\n",
    "# with open('X2_unet.pkl', 'wb') as f:\n",
    "#     pickle.dump(X_unet, f)\n",
    "# with open('X2_resnet.pkl', 'wb') as f:\n",
    "#     pickle.dump(X_resnet, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "58a1897873af96525b665e97a8c382e2d389dc353f2b36484e3d48d2d5c18126"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
